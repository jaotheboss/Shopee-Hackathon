{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report \nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\n\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":43,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input pipeline that delivers data for the next step before the current step has finished.\n# The tf.data API helps to build flexible and efficient input pipelines.\n# This document demonstrates how to use the tf.data \n# API to build highly performant TensorFlow input pipelines.\nAUTO = tf.data.experimental.AUTOTUNE\n# upload data into google cloud storage\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n# Configuration\nEPOCHS = 10\nBATCH_SIZE = 24 * strategy.num_replicas_in_sync\nMAX_LEN = 256\nMODEL = 'albert-xlarge-v2' # MODEL = 'bert-base-multilingual-cased'","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def _PREPROCESS(text):\n    \"\"\"\n    Function:     Acts as a sub function to the bigger _PREPROCESS.\n                  This function seeks to only preprocess the questions\n                     \n    Input:        Questions column\n       \n    Returns:      A column of reprocessed questions\n    \"\"\"\n    ## for manipulating the questions\n    stop_words = stopwords.words('english')\n    # stop_words.extend(['hi', 'hello', 'amp'])\n\n    #ps = PorterStemmer()\n    wnl = WordNetLemmatizer()\n\n    contractions = {\n              \"ain't\": \"am not / are not\",\n              \"aren't\": \"are not / am not\",\n              \"can't\": \"cannot\",\n              \"can't've\": \"cannot have\",\n              \"'cause\": \"because\",\n              \"could've\": \"could have\",\n              \"couldn't\": \"could not\",\n              \"couldn't've\": \"could not have\",\n              \"didn't\": \"did not\",\n              \"doesn't\": \"does not\",\n              \"don't\": \"do not\",\n              \"hadn't\": \"had not\",\n              \"hadn't've\": \"had not have\",\n              \"hasn't\": \"has not\",\n              \"haven't\": \"have not\",\n              \"he'd\": \"he had / he would\",\n              \"he'd've\": \"he would have\",\n              \"he'll\": \"he shall / he will\",\n              \"he'll've\": \"he shall have / he will have\",\n              \"he's\": \"he has / he is\",\n              \"how'd\": \"how did\",\n              \"how'd'y\": \"how do you\",\n              \"how'll\": \"how will\",\n              \"how's\": \"how has / how is\",\n              \"i'd\": \"I had / I would\",\n              \"i'd've\": \"I would have\",\n              \"i'll\": \"I shall / I will\",\n              \"i'll've\": \"I shall have / I will have\",\n              \"i'm\": \"I am\",\n              \"i've\": \"I have\",\n              \"isn't\": \"is not\",\n              \"it'd\": \"it had / it would\",\n              \"it'd've\": \"it would have\",\n              \"it'll\": \"it shall / it will\",\n              \"it'll've\": \"it shall have / it will have\",\n              \"it's\": \"it has / it is\",\n              \"let's\": \"let us\",\n              \"ma'am\": \"madam\",\n              \"mayn't\": \"may not\",\n              \"might've\": \"might have\",\n              \"mightn't\": \"might not\",\n              \"mightn't've\": \"might not have\",\n              \"must've\": \"must have\",\n              \"mustn't\": \"must not\",\n              \"mustn't've\": \"must not have\",\n              \"needn't\": \"need not\",\n              \"needn't've\": \"need not have\",\n              \"o'clock\": \"of the clock\",\n              \"oughtn't\": \"ought not\",\n              \"oughtn't've\": \"ought not have\",\n              \"shan't\": \"shall not\",\n              \"sha'n't\": \"shall not\",\n              \"shan't've\": \"shall not have\",\n              \"she'd\": \"she had / she would\",\n              \"she'd've\": \"she would have\",\n              \"she'll\": \"she shall / she will\",\n              \"she'll've\": \"she shall have / she will have\",\n              \"she's\": \"she has / she is\",\n              \"should've\": \"should have\",\n              \"shouldn't\": \"should not\",\n              \"shouldn't've\": \"should not have\",\n              \"so've\": \"so have\",\n              \"so's\": \"so as / so is\",\n              \"that'd\": \"that would / that had\",\n              \"that'd've\": \"that would have\",\n              \"that's\": \"that has / that is\",\n              \"there'd\": \"there had / there would\",\n              \"there'd've\": \"there would have\",\n              \"there's\": \"there has / there is\",\n              \"they'd\": \"they had / they would\",\n              \"they'd've\": \"they would have\",\n              \"they'll\": \"they shall / they will\",\n              \"they'll've\": \"they shall have / they will have\",\n              \"they're\": \"they are\",\n              \"they've\": \"they have\",\n              \"to've\": \"to have\",\n              \"wasn't\": \"was not\",\n              \"we'd\": \"we had / we would\",\n              \"we'd've\": \"we would have\",\n              \"we'll\": \"we will\",\n              \"we'll've\": \"we will have\",\n              \"we're\": \"we are\",\n              \"we've\": \"we have\",\n              \"weren't\": \"were not\",\n              \"what'll\": \"what shall / what will\",\n              \"what'll've\": \"what shall have / what will have\",\n              \"what're\": \"what are\",\n              \"what's\": \"what has / what is\",\n              \"what've\": \"what have\",\n              \"when's\": \"when has / when is\",\n              \"when've\": \"when have\",\n              \"where'd\": \"where did\",\n              \"where's\": \"where has / where is\",\n              \"where've\": \"where have\",\n              \"who'll\": \"who shall / who will\",\n              \"who'll've\": \"who shall have / who will have\",\n              \"who's\": \"who has / who is\",\n              \"who've\": \"who have\",\n              \"why's\": \"why has / why is\",\n              \"why've\": \"why have\",\n              \"will've\": \"will have\",\n              \"won't\": \"will not\",\n              \"won't've\": \"will not have\",\n              \"would've\": \"would have\",\n              \"wouldn't\": \"would not\",\n              \"wouldn't've\": \"would not have\",\n              \"y'all\": \"you all\",\n              \"y'all'd\": \"you all would\",\n              \"y'all'd've\": \"you all would have\",\n              \"y'all're\": \"you all are\",\n              \"y'all've\": \"you all have\",\n              \"you'd\": \"you had / you would\",\n              \"you'd've\": \"you would have\",\n              \"you'll\": \"you shall / you will\",\n              \"you'll've\": \"you shall have / you will have\",\n              \"you're\": \"you are\",\n              \"you've\": \"you have\"}\n\n    def contract(text):\n        for word in text.split():\n            if word.lower() in contractions:\n                text = text.replace(word, contractions[word.lower()])\n        return text\n       \n    def preprocess(text_column):\n        \"\"\"\n        Function:     This NLP pre processing function takes in a sentence,\n                      replaces all the useless letters and symbols, and takes \n                      out all the stop words. This would hopefully leave only \n                      the important key words\n                            \n        Input:        A list of sentences\n              \n        Returns:      A list of sentences that has been cleaned\n        \"\"\"\n        # Remove link,user and special characters\n        # And Lemmatize the words\n        new_review = []\n        for review in tqdm(text_column):\n            # this text is a list of tokens for the review\n            text = re.sub(\"w/\", 'with', str(review).lower()).strip()\n            text = re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9\\U00010000-\\U0010ffff]+\", ' ', text)\n            text = contract(text).split()\n                     \n            # Stemming and removing stopwords\n            # text = [wnl.lemmatize(i) for i in text if i not in stop_words]\n                     \n            new_review.append(' '.join(text))\n        return new_review\n       \n    text = preprocess(text)\n    return text","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen = 512):\n    \"\"\"\n    Function to encode the word\n    \"\"\"\n    # encode the word to vector of integer\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    \"\"\"\n    This function to build and compile Keras model\n    \n    \"\"\"\n    #Input: for define input layer\n    #shape is vector with 512-dimensional vectors\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # name is optional \n    sequence_output = transformer(input_word_ids)[0]\n    # to get the vector\n    cls_token = sequence_output[:, 0, :]\n    # define output layers\n    out = Dense(5, activation = 'softmax')(cls_token)\n    \n    # initiate the model with inputs and outputs\n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Nadam(lr=1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_data(data, n):\n    ref = {i: j for i, j in data['rating'].value_counts().reset_index().values.tolist()}\n    result = data.loc[data['rating'] == 1, :].sample(min(n, ref[1]))\n    for i in range(2, 6):\n        result = result.append(data.loc[data['rating'] == i, :].sample(min(n, ref[i])))\n    return result","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing dataset\ndata = pd.read_csv(\"/kaggle/input/shopee-sentiment-analysis/dataset/train.csv\")\ndata = split_data(data, 18500)\ndata = data.sample(frac = 1)\ntrain, val = train_test_split(data, test_size = 0.1)\ntest = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/dataset/test.csv')","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use the pre-trained model bert as a tokenizer \n#bert tokenizer has vocabulary for emoji. this is the reason we don't need to remove emoji from \n#datasets, for more details see the (EDA & data cleaning) notebook\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=685.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d0b2c8a4234b97ac59cebe3bc6097f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33249f4eeb604762975bdc9b742e1bb2"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n#call the function regular encode on for all the 3 dataset to convert each words after the tokenizer\n#into a vector\n#x_train,x_test, and x_validation will have the comment text column only,(in test called \"content\")\nx_train = regular_encode(_PREPROCESS(train.review.values), tokenizer, maxlen=MAX_LEN)\nx_val = regular_encode(_PREPROCESS(val.review.values), tokenizer, maxlen=MAX_LEN)\nx_test = regular_encode(_PREPROCESS(test.review.values), tokenizer, maxlen=MAX_LEN)\n\ny_train = train.rating.values - 1\ny_train = tf.keras.utils.to_categorical(y_train, 5)\ny_val = val.rating.values - 1\ny_val = tf.keras.utils.to_categorical(y_val, 5)","execution_count":51,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=82935.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75de77e803ed44f5b44f873a8e7e57d0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=9215.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f5f8a4281594c9c898aa6fc23093e4e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=60427.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5577a4ad6b42fcbddf07265daa7cd4"}},"metadata":{}},{"output_type":"stream","text":"\nCPU times: user 42 s, sys: 224 ms, total: 42.3 s\nWall time: 42.1 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a source dataset from your input data.\n# Apply dataset transformations to preprocess the data.\n# Iterate over the dataset and process the elements.\ntrain_dataset = (\n    tf.data.Dataset # create dataset\n    .from_tensor_slices((x_train, y_train)) # Once you have a dataset, you can apply transformations \n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)# Combines consecutive elements of this dataset into batches.\n    .prefetch(AUTO) #This allows later elements to be prepared while the current element is being processed.\n)\nvalid_dataset = (\n    tf.data.Dataset # create dataset\n    .from_tensor_slices((x_val, y_val)) # Once you have a dataset, you can apply transformations \n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)# Combines consecutive elements of this dataset into batches.\n    .prefetch(AUTO) #This allows later elements to be prepared while the current element is being processed.\n)\ntest_dataset = (\n    tf.data.Dataset# create dataset\n    .from_tensor_slices(x_test) # Once you have a dataset, you can apply transformations \n    .batch(BATCH_SIZE)\n)","execution_count":52,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# in the TPU\nwith strategy.scope():\n    #take the encoder results of bert from transformers and use it as an input in the NN model\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=251868920.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d24278f864f4cd3b7494ef46895ed97"}},"metadata":{}},{"output_type":"stream","text":"\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_word_ids (InputLayer)  [(None, 256)]             0         \n_________________________________________________________________\ntf_albert_model (TFAlbertMod ((None, 256, 2048), (None 58724864  \n_________________________________________________________________\ntf_op_layer_strided_slice_1  [(None, 2048)]            0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 10245     \n=================================================================\nTotal params: 58,735,109\nTrainable params: 58,735,109\nNon-trainable params: 0\n_________________________________________________________________\nCPU times: user 19.7 s, sys: 3.45 s, total: 23.2 s\nWall time: 25.9 s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\n# training the data and tune our model with the results of the metrics we get from the validation dataset\nn_steps = x_train.shape[0] // BATCH_SIZE\nval_steps = x_val.shape[0] // BATCH_SIZE\ntrain_history = model.fit(train_dataset, \n                          steps_per_epoch = n_steps, \n                          validation_data = valid_dataset,\n                          validation_steps = val_steps,\n                          epochs = EPOCHS,\n                          callbacks = [EarlyStopping(monitor='val_loss', min_delta = 0.01, patience = 1)])\n","execution_count":54,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n431/431 [==============================] - 630s 1s/step - loss: 1.4366 - accuracy: 0.3259 - val_loss: 1.1302 - val_accuracy: 0.4807\nEpoch 2/10\n431/431 [==============================] - 610s 1s/step - loss: 1.0774 - accuracy: 0.4979 - val_loss: 1.0586 - val_accuracy: 0.5065\nEpoch 3/10\n431/431 [==============================] - 610s 1s/step - loss: 1.0217 - accuracy: 0.5205 - val_loss: 1.0309 - val_accuracy: 0.5061\nEpoch 4/10\n431/431 [==============================] - 610s 1s/step - loss: 0.9939 - accuracy: 0.5327 - val_loss: 1.0151 - val_accuracy: 0.5244\nEpoch 5/10\n431/431 [==============================] - 610s 1s/step - loss: 0.9482 - accuracy: 0.5531 - val_loss: 1.0149 - val_accuracy: 0.5340\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Testing on validation first"},{"metadata":{"trusted":true},"cell_type":"code","source":"v_pred = model.predict(tf.data.Dataset.from_tensor_slices(x_val).batch(BATCH_SIZE), verbose = 1)\n\nv_pred = np.argmax(v_pred, axis = 1) + 1\n\nprint(classification_report(np.argmax(y_val, axis = 1) + 1, v_pred))","execution_count":55,"outputs":[{"output_type":"stream","text":"48/48 [==============================] - 21s 433ms/step\n              precision    recall  f1-score   support\n\n           1       0.70      0.73      0.72      1835\n           2       0.54      0.54      0.54      1800\n           3       0.57      0.51      0.54      1833\n           4       0.42      0.64      0.51      1885\n           5       0.44      0.24      0.31      1862\n\n    accuracy                           0.53      9215\n   macro avg       0.54      0.53      0.52      9215\nweighted avg       0.53      0.53      0.52      9215\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Exporting file"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = model.predict(test_dataset, verbose = 1)\npred = np.argmax(pred, axis = 1) + 1\nresult = pd.DataFrame({'review_id': test.review_id.values, 'rating': pred})\nresult.to_csv('submission_8 (BERT).csv', index = False)","execution_count":56,"outputs":[{"output_type":"stream","text":"315/315 [==============================] - 150s 477ms/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}